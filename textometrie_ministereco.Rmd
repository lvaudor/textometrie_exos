---
title: "Formation Textométrie"
output:
  html_document:
    df_print: paged
---

# Constitution du corpus de textes "ministereco"

Dans ce repo vous trouverez les traitements nécessaires pour **constituer un corpus de textes** ainsi que **le corpus ministereco** lui-même. Le rapport .Rmd comprend également quelques exemples de traitements textométriques qui peuvent faire l'objet de plus amples explications dans l'ouvrage [le Descriptoire](perso.ens-lyon.fr/lise.vaudor/Descriptoire/).

Le corpus de textes correspond à l'ensemble des **communiqués de presse** mis en ligne par le **Ministère de la Transition écologique et de la Cohision des territoires et Ministère de la Transition énergétique** à l'adresse suivante:

(https://www.ecologie.gouv.fr/presse)

# Packages

Voici les packages dont j'aurai besoin pour ce projet

```{r setup, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
library(tidyverse)
library(purrr)
library(rvest)
library(tidytext)
library(widyr)
```

# Web scraping et mise en forme des données

## récupérer les urls des communiqués de presse

Sur le site du Ministère de l'Ecologie:

```{r pages}
pages=paste0("https://www.ecologie.gouv.fr/presse?page=",1:493) 
pages[1:5]
```

## récupérer les urls des liens

Je définis une fonction (`get_comms()`) qui pour chaque page listée ci-dessus, me récupère l'ensemble des urls pointant vers les communiqués de presse: 

```{r def_recup_liens_comms}
recup_liens_comms <- function(page){
  Sys.sleep(runif(1,min=2,max=4))
  html <- read_html(page)
  urls <- html  %>% 
    html_nodes("body") %>% 
    html_nodes(".article") %>% 
    html_nodes("div") %>%
    html_nodes("h2") %>% 
    html_nodes("a") %>% 
    html_attr("href") %>% 
    paste0("https://www.ecologie.gouv.fr/",.)
  tib=tibble(urls=urls)
  return(tib)
}
```

J'applique ensuite itérativement la fonction `recup_liens_comms()` à l'ensemble des éléments du vecteur `pages` défini ci-dessus.

```{r recup_liens_comms}
if(!file.exists("data/liens.RDS")){
    liens=pages %>% 
      purrr::map_df(recup_liens_comms)
    saveRDS(liens,"data_raw/liens.RDS")
}
liens=readRDS("data/liens.RDS")
```

On va ensuite, pour chacun des liens, récupérer le contenu textuel de la page. Pour cela je définis une fonction `read_comm()`.

```{r def_read_comm}
read_comm=function(comm){
  Sys.sleep(runif(1,min=2,max=4))
  html <- read_html(comm)
  article <- html  %>% 
    html_nodes("body") %>% 
    html_nodes(".article") %>% 
    html_nodes("div")
  titre=article %>%
    html_nodes(".block-meem-page-title") %>% 
    html_text() %>% 
    stringr::str_replace_all("\\n"," ") %>%   # retire les sauts de ligne superflus.
    stringr::str_replace_all("\\s{2,15}"," ") # retire les espaces superflus
  date=article %>% 
    html_nodes(".posted") %>% 
    html_text() %>% 
    retire_sauts()
  sommaire=article %>% 
    html_nodes(".summary") %>% 
    html_text() %>% 
    retire_sauts()
  texte=article %>% 
    html_nodes(".content") %>% 
    html_nodes("p") %>% 
    html_text() %>% 
    paste(collapse=" ") %>% 
    retire_sauts()
  texte=paste(sommaire,texte)
  tib=tibble(lien=comm,
             titre=titre,
             date=date,
             texte=texte)
  return(tib)
}
```

L'exécution de `read_comm()` sur l'ensemble des liens (i.e. le web-scraping du contenu textuel) s'exécute en 2-3 heures:

```{r scraping}
if(!file.exists("data/ministereco_tib_docs.csv")){
  t1=Sys.time()
  tib_docs_raw=liens$urls %>%
    purrr::map(safely(read_comm))
  t2=Sys.time()-t1
  
  tib_docs_raw %>% 
    map("error") %>% 
    map_lgl(is.null) %>% 
    table()
  tib_docs=tib_docs_raw %>% 
    map_df("result")
  write_csv2(tib_docs,"data/ministereco_tib_docs.csv")
}
tib_docs=read_csv2("data/ministereco_tib_docs.csv") %>% 
 mutate(doc=paste0("doc",1:n())) %>% 
  mutate(date=lubridate::ymd(date)) 
```

## ajout métadonnée ministre

2012: ministère de l'Écologie, du Développement durable et de l'Énergie
2017: ministère de la Transition écologique et solidaire
2020: ministère de la Transition écologique
2022: ministère de la Transition écologique et de la Cohésion des territoires

On utilise la librairie `glitter` pour récupérer les dates d'exercice des ministres depuis Wikidata:

```{r recup_dates_ministres}
library("glitter")
ministres <- spq_init() %>%
  spq_add("?person p:P39 ?statement",.label="?person") %>% 
  spq_add("?statement ps:P39 wd:Q29962937") %>% 
  spq_add("?statement pq:P580 ?start") %>% 
  spq_add("?statement pq:P582 ?end") %>% 
  spq_select(-statement) %>% 
  spq_perform()
ministres
```

```{r assigne_ministre}
assigne_ministre=function(date_article){
  ministre=filter(ministres,date_article>=start,date_article<end) %>% 
    pull(personLabel)
  return(ministre)
}
tib_docs=tib_docs %>% 
  mutate(ministre=map_chr(date,assigne_ministre))
```

## écriture de tib_meta et tib_docs

```{r ecrit_tib_meta_tib_docs}
write_csv2(tib_docs,"data/ministereco_tib_docs.csv", quote="all")

tib_meta=tib_docs %>% select(-texte)
write_csv2(tib_meta,"data/ministereco_tib_meta.csv", quote="all")
```


## écriture de 5 txt

Pour exercice correspondant à lecture des données, utilisation de `tib_meta_N5` et 5 fichiers .txt correspondant aux textes bruts

```{r ecrit_textes_et_metadata}
write_text=function(ligne_docs){
  cat(ligne_docs$texte,
      file=paste0("data/",
                  ligne_docs$titre,
                  ".txt"))
}
read_text=function(ligne_docs){
  read_file(file=paste0("data/",
                  ligne_docs$titre,
                  ".txt"))
}

# l'espace en trop à la fin du titre est éliminé lors de la relacture de tib_meta depuis un csv
for (i in 1:5){
  write_text(tib_docs[i,])
}
```

# Traitement du langage naturel 

## traitement particulier pour le français

On ajoute un espace après les apostrophes pour permettre la tokénisation par la suite.

```{r apostrophes_suivies_dun_espace}
tib_textes <- tib_docs %>% 
  mutate(texte=str_replace_all(texte,"[\\'\\’]","\\' "))
```

## isolation des noms propres

```{r identifie_noms_propres}
bloc_noms_propres=function(texte){
  Avant="(?<=[^\\.\\?\\!\\;]\\s)"
  Nom="[:upper:]+[:lower:]*"
  resultat=str_replace_all(texte,
                           glue::glue("{Avant}({Nom})"),
                           "_\\1_")
  resultat=str_replace_all(texte,
                           glue::glue("^({Nom})"),
                           "_\\1_")
  resultat=str_replace_all(resultat,
                           "(?<=[^\\.\\?\\!\\;]\\s)(_[:upper:]+[:lower:]*)_\\s_([:upper:]+[:lower:]*_)",
                           "\\1_\\2")
  resultat=str_replace_all(resultat,
                           "(?<=[^\\.\\?\\!\\;]\\s)(_[:upper:]+[:lower:]*_[:upper:]+[:lower:]*)_\\s_([:upper:]+[:lower:]*_)",
                           "\\1_\\2")
  resultat=str_replace_all(resultat,
                           "(?<=[^\\.\\?\\!\\;]\\s)(_[:upper:]+[:lower:]*_[:upper:]+[:lower:]*_[:upper:]+[:lower:]*)_\\s_([:upper:]+[:lower:]*_)",
                           "\\1_\\2")
  
  return(resultat)
}
bloc_noms_propres("blablabla France gnagna")
bloc_noms_propres("Barbara blabla blabla")
bloc_noms_propres("blablabla. Merci de gnagna")
bloc_noms_propres("blablabla Barbara Pompili gnagna")
bloc_noms_propres("blablabla Région Auvergne Rhône Alpes gnagna")

tib_textes=tib_textes %>% 
  mutate(titre=map_chr(titre,bloc_noms_propres),
         texte=map_chr(texte,bloc_noms_propres))

write_csv2(tib_textes,"data/ministereco_tib_textes.csv")
```

## tokenization

Je crée la table `tib_mots` à partir de `tib_textes`, en tokenisant en mots (j'en profite pour lui demander de tout mettre en minuscules avec l'argument `to_lower=TRUE`.

```{r tokenize_instructions}
tib_mots <- unnest_tokens(tib_textes,
                          output="word",
                          input="texte",
                          to_lower=TRUE)
```


## lemmatisation

Je procède à la lemmatisation. Ici je me suis contentée de lemmatiser en utilisant la base de données Lexique382 (j'ai téléchargé ces données sur [le site de Lexique](http://www.lexique.org/telLexique.php)), puis en réalisant une jointure.

Voici un extrait de la table lexique382 (ici je ne montre que les variables qui m'intéressent ici)

```{r lexique382}
lexicon_fr=mixr::get_lexicon("fr")

tib_lemmes=tib_mots %>% 
  left_join(lexicon_fr,by="word") %>% 
  filter(!is.na(lemma)) %>% 
  filter(type %in% c("nom","ver","adj")) %>% 
  select(doc,word,lemma,type)

head(tib_lemmes %>% 
       select(word,lemma,type))
```


```{r save_tib_lemmes}
write_csv2(tib_lemmes,"data/ministereco_tib_lemmes.csv")
```

# Structural topic modelling

```{r}
tib_sparse=tib_lemmes %>% 
  group_by(lemma) %>% # compte pour chaque lemme...
  mutate(n=n()) %>% # ...son nombre d'occurrences puis
  filter(n>20) %>%  # retire ceux représentés moins de 20 fois dans le corpus
  ungroup() %>% 
  cast_sparse(row=doc, column=lemma, value=n)

dim(tib_sparse)
```

```{r calc_stm}
library(stm)
set.seed(123)

topic_model<-stm(tib_sparse,K=15, verbose=FALSE)
```

```{r save_topic_model, echo=FALSE}
saveRDS(topic_model,"data/ministereco_topic_model.RDS")
```
